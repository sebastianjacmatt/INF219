{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Backpropagation for MyNet with MSE loss L = sum( (y - y_pred)^2 ).\n",
    "    This matches the formulas:\n",
    "      δ^[L]_i = -2 * (y_i - y_pred_i) * f'^[L]_i(z^[L]_i)\n",
    "      δ^[l]_i = ( sum_{k} δ^[l+1]_k * w^[l+1]_k,i ) * f'^[l]_i(z^[l]_i)\n",
    "      ∂L/∂w^[l]_i,j = δ^[l]_i * a^[l-1]_j\n",
    "      ∂L/∂b^[l]_j = δ^[l]_j\n",
    "    summed over the batch dimension where appropriate.\n",
    "    \"\"\"\n",
    "    if not isinstance(model, MyNet):\n",
    "        raise ValueError(\"model must be an instance of MyNet\")\n",
    "    \n",
    "    L = model.L\n",
    "\n",
    "    # d[l] will hold δ^[l] = ∂L/∂z^[l].\n",
    "    d = {}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1) Compute local gradients at the output layer L\n",
    "    #    Using e'_i = -2 (y_i - y_pred_i) and chain‐rule\n",
    "    # ------------------------------------------------\n",
    "    # d[L] has shape (batch_size, n_l[L])\n",
    "    e_prime = -2 * (y_true - y_pred)                  # shape = (batch_size, n_l[L])\n",
    "    f_prime = model.df[L](model.z[L])                 # f'^[L](z^[L])\n",
    "    d[L]    = e_prime * f_prime\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2) Backpropagate through the hidden layers in reverse order\n",
    "    #    δ^[l] = (δ^[l+1] @ W^[l+1]) ⊙ f'^[l](z^[l])\n",
    "    # ---------------------------------------------------------\n",
    "    for l in reversed(range(1, L)):\n",
    "        W_next = model.fc[str(l+1)].weight.data  # shape: (n_l[l+1], n_l[l])\n",
    "        d[l] = (d[l+1] @ W_next) * model.df[l](model.z[l])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) Finally, compute ∂L/∂W^[l] and ∂L/∂b^[l] for each layer\n",
    "    #    ∂L/∂W^[l]_ij = sum_batch( δ^[l]_i * a^[l-1]_j )\n",
    "    #    ∂L/∂b^[l]_j  = sum_batch( δ^[l]_j )\n",
    "    # ---------------------------------------------------------\n",
    "    for l in range(1, L+1):\n",
    "        # a[l-1] shape = (batch_size, n_l[l-1])\n",
    "        # d[l]   shape = (batch_size, n_l[l])\n",
    "        # => d[l].T @ a[l-1] has shape (n_l[l], n_l[l-1])\n",
    "        model.dL_dw[l] = d[l].T @ model.a[l-1]     # sum over batch automatically\n",
    "        model.dL_db[l] = d[l].sum(dim=0)          # shape = (n_l[l],)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.78MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 360kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianmatthews/anaconda3/envs/INF265/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3333, 0.3333, 0.3333, 0.3333, 0.3333]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.5000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.5000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.5000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.5000\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
